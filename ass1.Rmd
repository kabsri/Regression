---
title: "Assignment 1"
author: "Kabilan Sriranjan"
date: "October 9th, 2017"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

##Question 1
##Dataset 1

```{r}
file1 = "http://www.math.mcgill.ca/yyang/regression/data/a1-1.txt"
data1 = read.table(file1, header = TRUE)
model1 = lm(data1$y ~ data1$x)
coef(model1)
plot(data1$x, data1$y, pch=16, xlab="X", ylab="Y")
title("Dataset 1")
abline(coef(model1), col=4)
```

This is the plot of the first data set with the regression line superimposed. We can also see the estimated coefficients.

```{r}
plot(data1$x, residuals(model1), pch=16, xlab="X", ylab="Residuals")
title("Dataset 1 Residuals")
abline(h=0, lty=2)
```

Here is the residual plot for the first data set. We can see that the mean of the data appears to be close to 0. Also if we ignore the two minor outliers at the bottom, all the data points are within a distance of 4 from the y=0 line, suggesting that the variance is approximately constant. Together this shows that the linear model is adequate for this data.

##Dataset 2
```{r}
file2 = "http://www.math.mcgill.ca/yyang/regression/data/a1-2.txt"
data2 = read.table(file2, header = TRUE)
model2 = lm(data2$y ~ data2$x)
coef(model2)
plot(data2$x, data2$y, pch=16, xlab="X", ylab="Y")
title("Dataset 2")
abline(coef(model2), col=4)
```


This is the plot of the second data set with the regression line superimposed.

```{r}
plot(data2$x, residuals(model2), pch=16, xlab="X", ylab="Residuals")
title("Dataset 2 Residuals")
abline(h=0, lty=2)
```

Here is the residual plot for the second data set. This data points are strongly concentrated near the y=0 line, with a uniform variance throughout. These two facts give reason to believe that the linear model is a great fit for this plot.


##Dataset 3
```{r}
file3 = "http://www.math.mcgill.ca/yyang/regression/data/a1-3.txt"
data3 = read.table(file3, header = TRUE)
model3 = lm(data3$y ~ data3$x)
coef(model3)
plot(data3$x, data3$y, pch=16, xlab="X", ylab="Y")
title("Dataset 3")
abline(coef(model3), col=4)
```


This is the plot of the third data set with the regression line superimposed.

```{r}
plot(data3$x, residuals(model3), pch=16, xlab="X", ylab="Residuals")
title("Dataset 3 Residuals")
abline(h=0, lty=2)
```

Here is the residual plot for the third data set. Most of the data points lie slightly below the y=0 line, while there are a few points that are far above it. The mean you should still balance out to around 0. At first, the variance appears to not be constant with x. However we could try removing the three outliers at the top and refitting, which would create a line more centered to the bulk of the data with a constant variance. Of the three datasets, the linear model seems to be the least adequate for this one.

##Question 2
$$
\widehat{\beta}_{1} = \frac{\sum_{i=0}^n y_ix_{i1} - \frac{\sum_{i=0}^ny_i\sum_{i=0}^nx_{i1}}{n}}{\sum_{i=0}^nx_{i1}^2 - \frac{(\sum_{i=0}^nx_{i1})^2}{n}}
$$
$$
\widehat{\beta}_0 = \frac{\sum_{i=0}^ny_i - \widehat{\beta}_{1}\sum_{i=0}^nx_{i1}}{n}
$$

##a)
We analyze the effects of a shift by replacing $x_{i1}$ with $x_{i1}-m$ and seeing what the new estimators are 

$$
\widehat{\beta}_{1}^{shift} = \frac{\sum_{i=0}^n y_i(x_{i1}-m) - \frac{\sum_{i=0}^ny_i\sum_{i=0}^n(x_{i1}-m)}{n}}{\sum_{i=0}^n(x_{i1}-m)^2 - \frac{(\sum_{i=0}^n(x_{i1}-m)^2)}{n}}
$$
$$
=\frac{\sum_{i=0}^n y_ix_{i1} - m\sum_{i=0}^ny_i- \frac{\sum_{i=0}^ny_i(\sum_{i=0}^nx_{i1}-nm)}{n}}{\sum_{i=0}^nx_{i1}^2 -2m\sum_{i=0}^nx_{i1} + nm^2 - \frac{(\sum_{i=0}^nx_{i1}-nm)^2}{n}}
$$
$$
=\frac{\sum_{i=0}^n y_ix_{i1} - m\sum_{i=0}^ny_i- \frac{\sum_{i=0}^ny_i(\sum_{i=0}^nx_{i1}-)}{n} + m\sum_{i=0}^ny_i}{\sum_{i=0}^nx_{i1}^2 -2m\sum_{i=0}^nx_{i1} + nm^2 - \frac{(\sum_{i=0}^nx_{i1})^2}{n} + 2m\sum_{i=0}^nx_{i1} - nm^2 }
$$
$$
= \frac{\sum_{i=0}^n y_ix_{i1} - \frac{\sum_{i=0}^ny_i\sum_{i=0}^nx_{i1}}{n}}{\sum_{i=0}^nx_{i1}^2 - \frac{(\sum_{i=0}^nx_{i1})^2}{n}}
$$
$$
=\widehat{\beta}_{1} 
$$

$$
\widehat{\beta}_{0}^{shift} = \frac{\sum_{i=0}^ny_i - \widehat{\beta}_{1}\sum_{i=0}^n(x_{i1}-m)}{n}
$$
$$
\frac{\sum_{i=0}^ny_i - \widehat{\beta}_{1}\sum_{i=0}^nx_{i1}}{n} + m\widehat{\beta}_{1}
$$
$$
=\widehat{\beta}_{0} + m\widehat{\beta}_{1}
$$

##b)
We analyze the effects of a rescaling by replacing $x_{i1}$ with $lx_{i1}$ and seeing what the new estimators are

$$
\widehat{\beta}_{1}^{rescale} = \frac{\sum_{i=0}^n y_ilx_{i1} - \frac{\sum_{i=0}^ny_i\sum_{i=0}^nlx_{i1}}{n}}{\sum_{i=0}^n(lx_{i1})^2 - \frac{(\sum_{i=0}^nlx_{i1})^2}{n}}
$$
$$
=\frac{l\sum_{i=0}^n y_ix_{i1} - l\frac{\sum_{i=0}^ny_i\sum_{i=0}^nx_{i1}}{n}}{l^2\sum_{i=0}^n(x_{i1})^2 - \frac{l^2(\sum_{i=0}^nx_{i1})^2}{n}}
$$
$$
=\frac{\sum_{i=0}^n y_ix_{i1} - \frac{\sum_{i=0}^ny_i\sum_{i=0}^nx_{i1}}{n}}{l(\sum_{i=0}^n(x_{i1})^2 - \frac{(\sum_{i=0}^nx_{i1})^2}{n})}
$$
$$
=\frac{1}{l}\widehat{\beta}_1
$$

$$
\widehat{\beta}_{0}^{rescale} = \frac{\sum_{i=0}^ny_i - \frac{1}{l}\widehat{\beta}_{1}\sum_{i=0}^nlx_{i1}}{n}
$$
$$
=\frac{\sum_{i=0}^ny_i - \widehat{\beta}_{1}\sum_{i=0}^nx_{i1}}{n}
$$
$$
=\widehat{\beta}_0
$$

##c)
We now compute the expectation and variance of all the new estimators. First for the shifted data: 
$$
\mathbb{E}[\widehat{\beta}_1^{shift}|\mathbf{X}]
$$
$$
=\mathbb{E}[\widehat{\beta}_1|\mathbf{X}]
$$
$$
=\beta_1
$$

$$
\mathbb{E}[\widehat{\beta}_0^{shift}|\mathbf{X}]
$$
$$
= \mathbb{E}[\widehat{\beta}_0+m\widehat{\beta}_1|\mathbf{X}]
$$
$$
=\mathbb{E}[\widehat{\beta}_0|\mathbf{X}] + m\mathbf{E}[\widehat{\beta}_1|\mathbf{X}]
$$
$$
=\beta_0 + m\beta_1
$$

$$
Var(\widehat{\beta}_1^{shift}|\mathbf{X})
$$
$$
=Var(\widehat{\beta}_1|\mathbf{X})
$$
$$
=\frac{\sigma^2}{S_{xx}}
$$

$$
Var(\widehat{\beta}_0^{shift}|\mathbf{X})
$$
$$
=Var(\widehat{\beta}_0+m\widehat{\beta}_1|\mathbf{X})
$$
$$
=Var(\widehat{\beta}_0) + m^2Var(\widehat{\beta}_1|\mathbf{X}) +2mCov(\widehat{\beta}_0, \widehat{\beta}_1)
$$
$$
= \sigma^2(\frac{1}{n} + \frac{\bar{x}_1}{S_{xx}}) + m^2\frac{\sigma^2}{S_{xx}} -2m\frac{\bar{x}_1\sigma^2}{S_{xx}}
$$
Note that the statistical properties of $\widehat{\beta}_1$ remain unchanged while for $\widehat{\beta}_0$ the expectation was shifted by $m\beta_1$ and the variance by $m^2\frac{\sigma^2}{S_{xx}} -2m\frac{\bar{x}_1\sigma^2}{S_{xx}}$  

Now we will do the computations for the recaled data:

$$
\mathbb{E}[\widehat{\beta}_0^{rescale}|\mathbf{X}]
$$
$$
=\mathbb{E}[\widehat{\beta}_0|\mathbf{X}]
$$
$$
=\beta_0
$$

$$
\mathbb{E}[\widehat{\beta}_1^{rescale}|\mathbf{X}]
$$
$$
=\mathbb{E}[\frac{1}{l}\widehat{\beta}_1|\mathbf{X}]
$$
$$
=\frac{1}{l}\beta_1
$$

$$
Var(\widehat{\beta}_0^{rescale}|\mathbf{X})
$$
$$
=Var(\widehat{\beta}_0|\mathbf{X})
$$
$$
=\sigma^2(\frac{1}{n} + \frac{\bar{x}_1}{S_{xx}})
$$

$$
Var(\widehat{\beta}_1^{rescale}|\mathbf{X})
$$
$$
=Var(\frac{1}{l}\widehat{\beta}_1|\mathbf{X})
$$
$$
=\frac{\sigma^2}{l^2S_{xx}}
$$

Note that the the properties of $\widehat{\beta}_0$ remain unchanged while for $\widehat{\beta}_1$ the expectation was rescaled by $\frac{1}{l}$ and the variance by $\frac{1}{l^2}$

##533 Question


We will find $\widehat{\beta}_0^c$ and $\widehat{\beta}_1^c$ satisfying the modified criterion. We know from calculus that $\widehat{\beta}_0^c$ and $\widehat{\beta}_1^c$ must make the partial derivatives equal to zero. Thus they must satisfy the following equations: 

$$
-2\sum_{i=1}^N[y_i - \widehat{\beta}_0^c - (x_{i1}-\bar{x}_1)\widehat{\beta}_1^c] = 0
$$
and
$$
-2\sum_{i=1}^N(x_{i1}-\bar{x}_1)[y_i-\widehat{\beta}_0^c - (x_{i1}-\bar{x}_1)\widehat{\beta}_1^c] + 2\lambda\widehat{\beta}_1^c = 0
$$

Using the fact that $\sum_{i=1}^N[x_{i1}-\bar{x}_1] = 0$ we can simplify the above two equations to:

$$
\widehat{\beta}_0^c = \bar{y}
$$
and
$$
\widehat{\beta}_1^c = \frac{\sum_{i=1}^Ny_i(x_{i1}-\bar{x}_1)}{\sum_{i=1}^N(x_{i1}-\bar{x}_1)^2 + \lambda}
$$

We now want to see how $\widehat{\beta}_0^c$ and $\widehat{\beta}_1^c$ can be used to find $\widehat{\beta}_0$ and $\widehat{\beta}_1$ satisfying the ridge criterion. Namely, we look for $\widehat{\beta}_0$ and $\widehat{\beta}_1$ satisfying the following equations:

$$
-2\sum_{i=1}^N[y_i - \widehat{\beta}_0^c - x_{i1}\widehat{\beta}_1^c] = 0
$$
and
$$
-2\sum_{i=1}^Nx_{i1}[y_i - \widehat{\beta}_0^c - x_{i1}\widehat{\beta}_1^c] + 2\lambda\widehat{\beta}_1^c= 0
$$

From question 2a, we suspect that we may want to use $\widehat{\beta}_1 = \widehat{\beta}_1^c$ and $\widehat{\beta}_0 = \widehat{\beta}_0^c - \bar{x}_1\widehat{\beta}_1^c$. Clearly this satisfies the first equation. I'm pretty sure it also satisfies the second equation but I dont't know how to show it.


##Extra Credit Question
Minimizing $\sqrt{\sum_{i=1}^N(y_i - \beta_0 - x_{i1}\beta_1)^2}$ is the same as minimizing $\sum_{i=1}^N(y_i - \beta_0 - x_{i1}\beta_1)^2$ by monotonicity of the square root function. We therefore want to find $\frac{\partial S}{\partial \beta_0}$ and $\frac{\partial S}{\partial \beta_1}$ and find the arguments that make them equal to 0.

$$
\frac{\partial S}{\partial \beta_0} = -2\sum_{i=1}^N(y_i-\beta_0-x_{i1}\beta_1) = 0
$$
$$
\implies \sum_{i=1}^Ny_i - n\beta_0 - \beta_1(\sum_{i=1}^Nx_{i1}) = 0
$$
$$
\implies n\beta_0 + (\sum_{i=1}^Nx_{i1})\beta_1 = \sum_{i=1}^Ny_i
$$

$$
\frac{\partial S}{\partial \beta_1} = -2\sum_{i=1}^Nx_{i1}(y_i-\beta_0-x_{i1}\beta_1) = 0
$$
$$
\implies \sum_{i=1}^Nx_{i1}y_i - \beta_0\sum_{i=1}^Nx_{i1} - \beta_1\sum_{i=1}^Nx_{i1}^2 = 0
$$
$$
\implies (\sum_{i=1}^Nx_{i1})\beta_0 + (\sum_{i=1}^Nx_{i1}^2)\beta_1 = \sum_{i=1}^Nx_{i1}y_i
$$

We now have a system of two linear equations, which is trivial to solve. We isolate $\beta_0$ in the first one:

$$
\beta_0 = \frac{\sum_{i=1}^Nx_{i1}y_i - (\sum_{i=1}^Nx_{i1}^2)\beta_1}{n}
$$
Substitute into the second equation to solve for $\beta_1$:

$$
\beta_1 = \frac{\sum_{i=0}^n y_ix_{i1} - \frac{\sum_{i=0}^ny_i\sum_{i=0}^nx_{i1}}{n}}{\sum_{i=0}^nx_{i1}^2 - \frac{(\sum_{i=0}^nx_{i1})^2}{n}}
$$
And finally substitute back into first equation:

$$
\beta_0 = \frac{\sum_{i=0}^ny_i - \widehat{\beta}_{1}\sum_{i=0}^nx_{i1}}{n}
$$

And so we have derived the formulas for $\beta_0$ and $\beta_1$.

We now take a look at the least absolute difference estimators and see what properties we can deduce. The derivative is fairly complicated to write, as it is a sum of piecewise functions. An important thing to note is that $\frac{\partial S}{\partial \beta_0}$ is just a sum of $N$ terms where each term is either 1 or -1. We only need to choose $\beta_0$ and $\beta_1$ such that the number of 1s and -1s is the same to make it equal to 0. To get $\frac{\partial S}{\partial \beta_1}=0$ we need to choose $beta_0$ and $\beta_1$ such that the sum of the $x_{i1}$'s that are above the regression line is equal to the sum that are below. 

For both types of regression, it is possible that no minimizer exists. One difference is that for the least absolute difference once, we may get multiple different estimator that all achieve the minimum.







